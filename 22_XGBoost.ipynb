{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "22 XGBoost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMBgq4RrrjEfK6vjjJiQVi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/DataSetsForML/blob/master/22_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "testewkkQnf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c5498514-7ee2-4f7b-d075-8e1f062f29d9"
      },
      "source": [
        "# Extreme Gradient Boosting\n",
        "from xgboost import XGBClassifier\n",
        "# instead of dataframe, we could have also worked on Numpy arrays directly! \n",
        "# XGBoost Requires Numpy arrays\n",
        "from numpy import loadtxt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = loadtxt('diabetes.csv', delimiter=',')\n",
        "#colnames = Pregnancies\tGlucose\tBloodPressure\tSkinThickness\tInsulin\tBMI\tDiabetesPedigreeFunction\tAge\tOutcome\n",
        "data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
              "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
              "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
              "       ...,\n",
              "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
              "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
              "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHsAMW5_YoP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# break your data into input and output\n",
        "x = data[:,0:8] # all columns including 0, excluding 8\n",
        "y = data[:,8] \n",
        "#y[:10]\n",
        "# split into train/test\n",
        "xtrain,xtest, ytrain,ytest = train_test_split(x,y, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsMHXeKmQ6W-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "9bae9a11-675c-4e52-a63d-73268d705026"
      },
      "source": [
        "model = XGBClassifier(n_estimators=150, learning_rate=1e-3)\n",
        "history = model.fit(xtrain,ytrain)\n",
        "print(model)\n",
        "\n",
        "# the hyperparams to tweak\n",
        "# 1-> learning rate \n",
        "# 2-> n_estimators -> number of parallel trees that will be generated \n",
        "# 3-> n_jobs and nthread -> used for parallelization -> n_jobs parallel XGboost jobs, and nthread\n",
        "# controls multithreaded execution \n",
        "# 4-> missing -> handling missing value \n",
        "# 5-> objective -> kind of inference -> classification or regression \n",
        "# 6-> MAX depth -> helps control bias-variance tradeoff. Keeping max depth at 3 will also ensure\n",
        "# that a bagged tree is formed \n",
        "\n",
        "# njobs -> LONG (time) piece of code that should be given resources and should not impact\n",
        "# user tasks \n",
        "# nthread -> multiple threads to parallelize execution "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.001, max_delta_step=0, max_depth=3,\n",
            "              min_child_weight=1, missing=None, n_estimators=150, n_jobs=1,\n",
            "              nthread=None, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPH9Gi_va2TF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2ec9e9c-f860-490a-d2b4-c4ccb97815f4"
      },
      "source": [
        "predictions = model.predict(xtest)\n",
        "accuracy = accuracy_score(ytest, predictions)\n",
        "accuracy\n",
        "\n",
        "#predictions = [round(value) for value in predictions]\n",
        "#predictions\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7597402597402597"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nks6V7Tbzw4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "75fd320e-bc53-4d5e-da0f-7382a2387714"
      },
      "source": [
        "# with lr=1e-2\n",
        "model = XGBClassifier(learning_rate=1e-2)\n",
        "history = model.fit(xtrain,ytrain)\n",
        "predictions = model.predict(xtest)\n",
        "accuracy = accuracy_score(ytest, predictions)\n",
        "accuracy\n",
        "# to get better accuracy, tune Hyperparameters via GRID search (exhaustive) or Random Search (for minimal workable \n",
        "# accuracy score)\n",
        "\n",
        "# Grid -> when hyperparameter search space is finite (limited values can be supplied)\n",
        "# SARIMAX, ARIMA, KNN, SVM, batch_size -> finite hyperparams\n",
        "# Random -> infinite (when any number could have been the right answer!)\n",
        "# EPOCHS, input-output dimensions, Learning rate -> infinite solutions possible!\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7727272727272727"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkiQUw34StZC",
        "colab_type": "text"
      },
      "source": [
        "TREES-> Bagging on individual trees, Boosted Trees was the way of selecting multiple trees ONE after the OTHER, such that each tree performance is better than the previous selection.  \n",
        "\n",
        "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n",
        "\n",
        "https://shirinsplayground.netlify.com/2018/11/ml_basics_gbm/ \n",
        "\n",
        "XGBoost-> Extreme Gradient Boosting\n",
        "\n",
        "1) C++ implementation redone in python thus VERY high performance due to parallelization \n",
        "\n",
        "2) v/s other packages-> it has almost always outperformed other gradient boosting algorithms \n",
        "\n",
        "3) choice of hyperparameters is HUGE-> this makes it much more customizable than other models \n",
        "\n",
        "AT the CORE-> it's still gradient boosting (it can be ascend or descend!) \n",
        "\n",
        "Regression Example:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
        "\n",
        "Without a cluster (of machines or GPU), XGBoost would have performed same as its contempory algos from ScikitLearn or Keras \n",
        "\n",
        "Keras -> 1 layer of Neural Network is actually machine learning! \n",
        "https://playground.tensorflow.org\n",
        "\n",
        "tf.estimator -> Machine learning API from tensorflow, contains most basic inference algorithms (like linear regression)\n",
        "\n",
        "But estimators together can form complex networks such as CNNs!\n",
        "https://towardsdatascience.com/first-contact-with-tensorflow-estimator-69a5e072998d\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}