Dictionary
I am a good boy
I am good today/......


Word Frequency (PRE SORT)
I	34
am      32
a  	11
good	32
boy	43
today   42

Word Frequency RANK (POST SORT)
boy	43	1
today   42	2
I  	34	3
am	32	4
good	32	5
a  	11	6

word_index = {0:<PAD>,1:'<start>',..,4:'boy',5:'today'....'10':'a'}

I am a good boy   1
I am not feeling well 0
Today is bad 0
Good boy is today 1
1 + 0 /2 = 0.5
# MORE TRAINING I HAD -> better sentiment i could have calculated!
update_sentiment(prev,new): 
	return (prev+new)/2 # sum all /divide_total_count
I am sick -> 0 
# TEST 
# 27th
Web -> More worried about special cases
Data -> OUTLIERS -> drop them rather than worry
	EXCEPTION
	MAGIC NUMBERS
	USELESS DATA 

tea-> positive
tea-> negative -> PUBLIC -> 10,000-> 1200 dislike!

# NEVER manipulate -> collect more data 
# Never train with LESS data


updated_sentiment = prev+new / 2 = 1 + 0 / 2 -> 0.5
I am a thief -> 0
updated_sentiment =  0.5 + 0/2 = 0.25
Word Sentiment RANK (POST SORT)
boy	0.33	1
today   0.23	2
I  	0.212	3
am	0.25	4
good	0.1333	5
is  	0.110	6
bad     0.0003  7    
well	0.21	8

I am well 1

y = w1*0.33 + w2*0.45 + w3*0.21 + bias 
assume w1=w2=w3=bias=1
y = (0.33 + 0.45 + 0.21)/3 -> total sentiment of all word /average
y = (0.99/3) = 0.33 -> this is < 0.5 -> hence Negative sentiment

SENTIMENT ANALYSIS:
1) MAKE ALL SENTENCES INTO SAME SIZE? Neurals are hardcoded
2) BREAK SENTENCES INTO NUMBERS 
3) Train, testing data 
3) Learn Individual Word Sentiment, and hence of entire sentence
	a) Break Sentence into Words 
	b) calculate individual word sentiment
	c) sum all of that up, take average 

SENTIMENT(SENTENCE) = average (sent(w1),sent(w2)...sent(w256))
