{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20 Prep Part 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMspCnzmSZqgtiMIUSFhUeR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/DataSetsForML/blob/master/20_Prep_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igtRtQAiDEQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN v/s CNN\n",
        "\n",
        "# RNNs: Recurrent Layers -> Output becomes part of input (Electrical feedback loops). This may\n",
        "# happen on any type of layer. \n",
        "# y = f(x,y)\n",
        "# Sequence -> multiple items taken in a segment for pattern recog\n",
        "# Understanding patterns makes it highly overfitted towards data that happens in sequence\n",
        "# like audio(beats or tune), speech, NLP (word2vector), CAPTCHA (images happening again and again), TIme Series\n",
        "# Examples: LSTM, GRU\n",
        "# practical examples: time series analysis, translation, encoding/decoding, encry/dec,\n",
        "# word prediction, autocorrection, sentence completion, customer support-> escalation \n",
        "# genetic sequence detection, disease patterns, cell structure (nucleus and cytoplasm density)\n",
        "# \n",
        "# CNNs: Recurring Layers -> happening again and again -> partially connected layers that \n",
        "# learn pattern with help of filtering. Filter (Convolution) is multiplied into the input set \n",
        "# to generate Activation Map. This activation map serves as feature for the next layer.\n",
        "# y2 = f(y1(x))\n",
        "# Filtering -> Convolutions are multiplied into the data and passed through a 'relu' activation \n",
        "# Understanding minutest details resulting on the Activation maps, so that if initial object\n",
        "# was distorted, even those features are understood \n",
        "# Like: object detection, movement detection, computer vision, emotion analysis on NLP\n",
        "# Examples: ResNet, VggNet, AlexNet\n",
        "# practical examples: object detection, activity detection, finger/facial recognition\n",
        "# conveyer belts/smart car/security cameras, Text extraction from images \n",
        "\n",
        "# Fully Connected Layers-> Dense Layers\n",
        "# Dense layers are usually used towards input or output because while they are great at pattern\n",
        "# detection, they also are very expensive, hence their usage should be controlled via activation\n",
        "# functions inside the layer\n",
        "# Output layers are usually dense, but can be a different layer also, depending on your labels\n",
        "# Multi-Categorical output-> Dense('softmax')\n",
        "# Binary-Categorical output-> Dense('Sigmoid')\n",
        "# Regression (only pos output, like Age, items sold)-> Dense('ReLU')\n",
        "# Regression (neg acceptable, profit/loss, temperature)-> Dense (no activation)\n",
        "# Probability-> Dense('Sigmoid')\n",
        "# Value between -1 to +1 -> Dense ('tanh')\n",
        "\n",
        "\n",
        "# input layer is decided based on your data. \n",
        "# Images -> Flattening \n",
        "# Text -> Embedding \n",
        "# Numbers -> Dense \n",
        "\n",
        "# Partially Connected Layers-> Convolutional Layers\n",
        "# Convolutional Layers are usually inside hidden layers to convolute and filtrer on FEATURES\n",
        "# generated post data augmentation. Here, activation functions help with filtering and changing\n",
        "# scale of the data. Activation Fns are usually AFTER Conv layers, not inside. \n",
        "# Because they are not fully connected, long chains of Convolutional layers are required to \n",
        "# learn finer and more complex pattern with each block of Conv layer \n",
        "\n",
        "\n",
        "# Metrics to remember: Precision, Recall, Accuracy, Cross validation score, mean squared error\n",
        "# mean absolute error, F1 score, Area Under Curve (AUC), \n",
        "# https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/\n",
        "\n",
        "\n",
        "# Can be used in combination also (HIGHLY EXPERIMENTAL)\n",
        "# https://www.datasciencecentral.com/profiles/blogs/combining-cnns-and-rnns-crazy-or-genius\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh9u8_27K66-",
        "colab_type": "text"
      },
      "source": [
        "Multicollinearity in Linear Regression\n",
        "\n",
        "\n",
        "LINEAR Relationship-> increase or decrease \n",
        "together \n",
        "\n",
        "In breast cancer analysis -> Premiter and area were colinear with Radius! \n",
        "\n",
        "\n",
        "\n",
        "#What is it?\n",
        "More than one variable demonstrate very high linearity (tending -1 or +1)\n",
        "\n",
        "Multiple variables that could have done the same thing are now present (Radius, Area, Perimenter) \n",
        "\n",
        "All 3 are trying to calculate the size of cell, so may only 1 or 2 of them (radius along with one other) was good enough \n",
        "\n",
        "When variables are present it was same as saying that\n",
        "\n",
        "y = mx + c, where x = [x1, x2, x2 ,x2, x3 ,x4]!!!! Specially in time series, regressions, probabilistic algos, this becomes a HUGE challenge\n",
        "\n",
        "#Consequences?\n",
        "\n",
        "as we can observe this will lead to heavy bias towards the collinear features! model will be overfitted towards them!\n",
        "\n",
        "Please note it is a cell-> not a perfect circle! Often, area and perimeter may not be exactly pi-r-square or 2-pi-r! That will also cause model to be extremely confused and easily manipulatable. \n",
        "\n",
        "\n",
        "#What to do?\n",
        "1) Drop the recurring collinearities (keep one of them, drop other 2) \n",
        "\n",
        "2) PCA! dimensionality reduction will take care of the fact that most important dimensions are already absorbed on first axis!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Dimensionality Reduction Techniques\n",
        "\n",
        "1) PCA\n",
        "\n",
        "2) LDA \n",
        "\n",
        "3) Partial Least Squares Regression \n",
        "\n",
        "\n",
        "http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/152-principal-component-and-partial-least-squares-regression-essentials/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8DcBk3nxmG9",
        "colab_type": "text"
      },
      "source": [
        "1) identify one field of interest- either NLP, computer vision, robotics, animation, security, FinTech or audio\n",
        "\n",
        "2) Each of the above domain has its specific subjects \n",
        "\n",
        "3) Common subjects: <in the order> \n",
        "\n",
        "a) Math: Vectors, Matrices and scalars, Number theory, series/prop, probability, permutation/combination, advanced probability with dependencies and permutations, trignometry and geometry (basic, not advanced), Logarithmic (base 10, base 2, ln) , Discrete and Continuous math (advanced- ongoing process, should be done continously over a period of 2-3 years), Calculus (4-5 years- don't deep dive, should be side ongoing activity) (this if you want to calculate back-propogations, derivates, weights, gradies on your own) (at least basic integration and diff formula should be known), Graph Theory (advanced), Linear Algebra and equations, Induction (art of forming formulae) \n",
        "\n",
        "b) Math-Statistics: Economics (basics), profit-loss, Game Theory, Advanced Statistics Formula, Political Science, Chaos Theory, \n",
        "\n",
        "c) physics (advanced): Signal and Systems , Kinemetics, Relativity, electrical circuits (V=IR, kirchoff's law, fleming's rules, vector math, activation functions/clipping and rectification), force and momentum, inertia, basic understanding of newtown's law's of motion, Optics, thermodynamics (energy), Magnetism (electromagnetic principles, Field theory, parabola/hyperbola math), basic fissions and fusion principles, metrics-testing and evaluation techniques\n",
        "\n",
        "d) image processing fundamentals and color theory (solid, light, transparency)\n",
        "\n",
        "e) NLP-> whatever language you are expert at- it's grammar, linguistics, phonetics, programming, sentence structure, dictionary, tenses, part-of-speech tagging, named-entity and co-references\n",
        "\n",
        "f) Biology-> genetics, diseases, micro-biology\n",
        "\n",
        "g) other-> color theory, psychology, neural structures- axon and dendrites, electrical fuild nature\n",
        "\n",
        "h) DATA STRUCTURES-> VERY VERY CRITICAL. Array, List (linked list, doubly linked list, circular list), Dictionary, DataFrames, Tuples, Classes, Queues, Hashtables, TREES(binary search tree, red-black trees, weighted trees, (a)directed trees, acyclic and cyclic trees,  Tries) \n",
        "\n",
        "i) Computer science: computer organization, fundamentals of any 1 scripting and 1 programming language, RAM usage- stack and heap, POINTERS, static v/s dynamic programming, declarative v/s imperative programming, a little bit of datastores (SQL and NoSQL and XML and JSON and Hadoop and HDFS, Spark,), file formats, SEARCH and SORTING algorithms, STRING and NUMERICAL algorithms  \n",
        "\n",
        "\n",
        "j) Chemistry -> atomic structure [ionic, shared, covalent], electron rotation, metrics-testing and evaluation techniques"
      ]
    }
  ]
}