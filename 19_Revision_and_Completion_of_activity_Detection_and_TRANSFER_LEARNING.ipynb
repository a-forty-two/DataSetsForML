{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19 Revision and Completion  of activity Detection and TRANSFER LEARNING.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmzeEaVMUVRqiaOyPfHFXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/DataSetsForML/blob/master/19_Revision_and_Completion_of_activity_Detection_and_TRANSFER_LEARNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7PUz0znBOfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will use a hardcoded video as test data\n",
        "# if doing on laptop you can even use webcab for same program \n",
        "# as input stream \n",
        "\n",
        "# inputs were hardcoded,video is not a user input but an mp4 or avi file\n",
        "\n",
        "# Algorithm\n",
        "# 1. Input a video (or live stream)\n",
        "# 2. open the frames as images\n",
        "# 3. Run object detection on images to understand continous usage of objects (RESNET/VGG)\n",
        "# 4. Continous usage of objects is called ACTIVITY\n",
        "# 5. in each frame of video, classify the frame as an activity\n",
        "# 6. combine the frames back into a video to deliver as classified output! \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd5QlZ0FEjHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## start of: TRAIN.py\n",
        "\n",
        "# WHEN DEVELOPING-> import only when required\n",
        "# - minimize program size - control over 3rd party components\n",
        "# WHEN in production-> include all libraries in the beginning of program \n",
        "# itself\n",
        "# - minimize LATENCY - all mandatory components should be available before \n",
        "# execution starts \n",
        "# Sometimes all libraries are written in a single config or dependency\n",
        "# or include file\n",
        "# this is just to minimize \"typing\" size, no impact on program because\n",
        "# while execution, your library will be copy-pasted with its source. \n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pickle # to read/write binary files as dumps (for weights and biases or binarized labels)\n",
        "import cv2\n",
        "import os # os.separators to separate path names and get file name and file folder name\n",
        "from keras.applications import ResNet50\n",
        "# to customize models with own layers\n",
        "from keras.layers.core import Flatten, Dropout, Dense\n",
        "from keras.layers.pooling import AveragePooling2D # AVERAGE pixel -> do this when color of object shouldn't matter \n",
        "from keras.layers import Input # input layer can take general or any input size (dynamic input size but once decalred cannot be changed)\n",
        "from keras.models import Model\n",
        "#label binarizer to encode/decode binarized outputs \n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import imutils\n",
        "from imutils import paths\n",
        "# but what if i wanted to take average without losing channels?\n",
        "# Conv2D -> GlobalAveragePooling-> Conv2D would split data into R, G and B channels \n",
        "# then GAP will calculate averages for R, G, and B separately thus preserving colors \n",
        "from keras.optimizers import SGD\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xU9AiEvE4Qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variables that should have been collected from user through UI or command line or API post\n",
        "dataset = 'data'\n",
        "model_path = 'bin'\n",
        "binarizer_path = 'bin'\n",
        "evaluation_path = 'eval'\n",
        "test_path = 'test'\n",
        "# directory structure required\n",
        "# myactivitydetector.py --dataset data --bin bin --eval-data eval --test-data test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heEmQnzIHa_T",
        "colab_type": "code",
        "outputId": "d73b2680-e3d6-4365-98c2-17f921a524ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# shuffle the data after reading all paths\n",
        "\n",
        "imagePaths = sorted(list( paths.list_images(dataset)))\n",
        "random.seed(42)\n",
        "random.shuffle(imagePaths)\n",
        "imagePaths[:2]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/weightlifting/00000147.jpg', 'data/hockey/00000009.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R3or1VFHsVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to conver to img_to_array and resize \n",
        "# this is how to use ResNET \n",
        "# in our VGG-> we took (96,96,3)\n",
        "# larger images are required because smaller image= not enough filters possible\n",
        "label_names = [\"weightlifting\",\"swimming\",\"hockey\",\"basketball\"]\n",
        "labels = []\n",
        "data = []\n",
        "for path in imagePaths:\n",
        "  label = path.split(os.path.sep)[-2] # folder name! to represent class name \n",
        "  if label not in label_names:\n",
        "    continue\n",
        "  labels.append(label)\n",
        "  img = cv2.imread(path)\n",
        "  # ResNet requires this to be done- rearranging the channels and making sure its R,G,B in this format and not B,G,R\n",
        "  # ImageNet-> means -> separate means for R,G,and B!!! their channel positions are also hardcoded!\n",
        "  # so if this was not done to an image, the logic of R will be applied to B and vice versa!! \n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)   # these are simple reshape, resize and expand_dims functions\n",
        "  img = cv2.resize(img, (224,224)) \n",
        "  data.append(img)\n",
        "  # RESNET model -> input size (224,224,3) , only accept RGB channel, while most images\n",
        "  # like jpeg, png, jpg, giff are arranged in BGR (just opposite)\n",
        "  # Object detection -> there order of the color did  not matter because RGB values combined would have \n",
        "  # resulted in same color anyway \n",
        "  # just in case error handling-> if image is already RGB, the below function will do nothing, if its BGR, it will\n",
        "  # be converted to RGB \n",
        "  \n",
        "  # they readjust dimensions to fit various frameworks \n",
        "  # RESNET doesn't require image aug sep because it was inbuilt into its architecture \n",
        "  # aug.flow-> model.fit_generator \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gmybBVnJ_-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "lb = LabelBinarizer() # LABELS were converted into 1-hot matrices instead of label encoded (categorical label data) \n",
        "# if one-hot columns on input data-> ONE-HOT encoding (it generates new columns out of categorical input data)\n",
        "\n",
        "# 5 encoding-> LabelEncoders, One-Hot, Multi-Hot, LabelBinarizer, Word2Vector \n",
        "# Label Encoder-> no statistical relationship between data, and only positional representation was required\n",
        "#        Sentiment Analysis, Topic Understanding, Encryption/Decryption, Language/Machine translation \n",
        "#        Preserves word as a token, not meant for categorical data (but can be used in simple binary classification 0-1)\n",
        "\n",
        "\n",
        "# Customer_problem     Category_problem    SolutionSuggested (label)\n",
        "# lost my card          card-related             CardsTeam \n",
        "# missed my EMI         loan-related            LoansTeam\n",
        "# new credit card       card-related            CardsTeam\n",
        "# random deductions     card-related            Security Team       \n",
        "\n",
        "# customer problems are continous english statements -> for their encoding I have 2 choices \n",
        "#        Label Encoding, Word2Vector \n",
        "# category problem is an input categorical column (all values are INDEPENDET of each other within a column). This \n",
        "#       column should ideally be used for filtering. The best way of filtering with minimal programming-> one-hot encoding \n",
        "# Solution suggested is the LABEL or output. For its encoding I have 2 choices\n",
        "#        label encoder, Label Binarizer \n",
        "# If it is a prediction-on-input problem, like autocomplete, spell check, sentence completion, word prediction \n",
        "# I need a VECTORS capable of changing vectors when word is used in various positions and sequences \n",
        "\n",
        "# how do i select length or granularity of these vectors? Should be they sentence, words, characters, pages? \n",
        "# what shold be the segmentation size?\n",
        "# WHEN USING PATTERNS FOR PREDICTION-> large patterns detect small objects, small patterns detect large object\n",
        "# Smallest pattern in string is a CHARACTER\n",
        "# that's why character level Seqence-2-sequence mapping is preferred \n",
        "\n",
        "# when encoding \"I am going home\"\n",
        "# I's vector is independent-> because it is starting vector\n",
        "# but the vector for 'o' on 7th position and 13th position is built up of sequences just before them \n",
        "# the 7th position's vector has been calculated wrt 'I am go' and not just 'o' alone. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# LABEL BINARIZER is MATHEMATICALLY massively more optimized and faster to calculate than other data structures\n",
        "# MATRICES -> very close to NUMPY and thanks to numpy, their operations are very fast in most languages \n",
        "# Label Encoding CANNOT be used for MULTI-CLASSIFICATION but Label Binarizer can be! multiple columns hot on a single\n",
        "# row indicate multiple classification:   [ [1 0 1], [0 0 0], [ 1 0 1]] (multi columns hot)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# One-hot and Multi-hot encoding -> ONLY on INPUT categorical data. Why not regression? Because it will take all \n",
        "#        unique values, and make them columns! It is very likely that most values are unique!!! \n",
        "#        Sales predictions (region specific), FILTERING of input data, wherever input categories were independent of \n",
        "#         each other (profits in Russia cannot be predicted the same way as Europe!)\n",
        "#        Onehot or multi-hot encode-> FILTER the data or let weights be considered as 0 -> do individual learning on\n",
        "#         each encoded category seperately \n",
        "#        It is NOT used for word or char or sentence tokens! \n",
        "\n",
        "# Label Binarizer -> ONLY on OUTPUT (labelled) categorical data. Why not regression? same reason as one-hot encoded \n",
        "#         Scenarios to use it: When output categories are independent of each other, and input may carry more than\n",
        "#         one output (an image can contain multiple objects that can be detected)\n",
        "#         Object detection, activity detection, advanced multi-class classifications\n",
        "#         DIFF from One-hot-> done on output not input, does not create new columns in\n",
        "#                             dataset-> creates new columns in matrix instead\n",
        "\n",
        "\n",
        "\n",
        "labels = lb.fit_transform(labels)\n",
        "labels # HOT-encoded MATRIX \n",
        "(xtrain, xtest, ytrain, ytest) = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=42 )\n",
        "# << left shift operation\n",
        "#       0 0 0 1  0 0 0 0   -> left shift by 2 - >   0 1 0 0 0 0 0 0 \n",
        "#          -> right shift by 1 -> 0000 1000 \n",
        "# This label binarizing is EXTREMELY FAST because of LEFT shift operations which happen at bit-level \n",
        "# there is no need to go through the entire list of items, just need to know HOW many items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aath-DKpKGXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split into train/test\n",
        "# whenever we need classes balanced while splitting, we use a process called 'stratification'\n",
        "# it shuffles on the basis of types of output labels, and not just random_state\n",
        "(xtrain, xtest, ytrain, ytest) = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=42 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZXaD49nMPXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainAug = ImageDataGenerator(rotation_range=25, zoom_range=0.20, width_shift_range=0.2, height_shift_range=0.2, \n",
        "                              shear_range=0.20, horizontal_flip=True, fill_mode='nearest')\n",
        "\n",
        "valAug = ImageDataGenerator() # default augmentation (everything is set to false-> no augmentation)\n",
        "# this will help validate the model in 2 augmentation- one with our v/s with any other option! \n",
        "# these values are for R, G and B respectively. This is the reason why we had to make sure all images are \n",
        "# only RGB and not BGR. Else wrong mean values would have been considered for loss calculation. \n",
        "means = np.array( [ 103.939, 116.779, 123.68 ])\n",
        "trainAug.mean = means\n",
        "valAug.mean = means \n",
        "# deviations will be calculated from respective means\n",
        "# train data -> trainaug.means\n",
        "# valdata -> valAug.means \n",
        "\n",
        "# Since the data is augmented and no longer the original image, the difference between the original image (no aug- val)\n",
        "# and augmented image (trainAug) can be calculated as difference in the mean \n",
        "# when using keras.application \n",
        "# To find out this mean there are various formula, but we are going to use the preferred production method-\n",
        "# WITH RESNET and VGG, we use means from 'ImageNet'. \n",
        "# 3 averages-> 1 each for R, G and B \n",
        "# From scratch way is to calculate it based on your data. BUT, when doing transfer learning, follow what model\n",
        "# says. VGG and Restnet for 'ImageNet' standards. \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUih1xP-UK4A",
        "colab_type": "code",
        "outputId": "d777acde-e54d-4a02-fe25-2d7b998c269a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "# LAST time-> we built our own PandaVGG, THIS time-> transfer learning using ResNet \n",
        "# Last time-> we calculated our weights for PandaVGG, THIS TIME-> preserved ResNet's weights by setting \n",
        "# non trainable layers \n",
        "# REST of it is EXACTLY the same except hyperparameters \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# cloud -> hadoop (parquet) -> g(s3) -> no bigquery\n",
        "# PySpark notebooks -> external tables -> DataBricks \n",
        "# Transfer learning and build our model on top of it\n",
        "basemodel = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(224,224,3)))\n",
        "# include top = False -> i am going to provide my own top-layers, the default ResNet top layers are not requried\n",
        "# (top layers are the ones on right)\n",
        "# name: Left side of model is called BASE or BOTTOM, the right side of model is called a HEAD or TOP\n",
        "\n",
        "# if i am importing my weights from ImageNet-> then during learning, all this will be recalculated and transfer-learning\n",
        "# will be lost!!!! That is why, when Transfer Learning, ALWAYS make sure that transfered layers' learning is set\n",
        "# to false \n",
        "\n",
        "# activity detection and classification model\n",
        "# use object detection as base algorithm and build your own algo on top of it \n",
        "# basemodel was loaded as KNOWLEDGE, myModel is the 'Head' or 'Top' model \n",
        "myModel = basemodel.output # TRANSFER LEARNING (Knowledge transfer) is done \n",
        "# another of adding layers besides model.add\n",
        "# MATMUL way -> multiplicative way of adding layers (because anyway between layers its the mul, inside layers-> add)\n",
        "# model  = (additive_layers)(model)\n",
        "# MATMUL is nump matrix multiplication \n",
        "# NEURAL -> playing with TENSORS-> only 2 ops-> Add (inside a layer), Mul (between the layers) \n",
        "myModel = AveragePooling2D(pool_size=(7,7))(myModel) # in matmul AXB != BXA, same as model.add(AvgPool2D)\n",
        "#Pattern recognition \n",
        "# till now my data is 2 d! Image Pattern Recog\n",
        "# Flatten -> Dense(ReLu) -> Dense (Softmax)\n",
        "\n",
        "\n",
        "# a= a+b, a+=b <- diff methods of doing the same thing\n",
        "# a++, a=a+1\n",
        "\n",
        "# Add, Mul -> y = weights*x + bias \n",
        "# y = w1*x1 + w2*x2 + w3*x3 + bias \n",
        "\n",
        "# model.add () -> this is not addition, this means add the layer next to it (which would be multiplication)\n",
        "\n",
        "\n",
        "\n",
        "# ways to add layers in Keras\n",
        "# 1. Model.add\n",
        "# 2. MatMul way -> either add inside layer or multiply between layers \n",
        "# 3. Class Derivation -> SIMILAR approach to what we did with VGG except this also includes Inheritence \n",
        "# from keras' model class \n",
        "\n",
        "#class MiniVGGNetModel(Model):\n",
        "#\tdef __init__(self, classes, chanDim=-1):\n",
        "#\t\t# call the parent constructor\n",
        "#\t\tsuper(MiniVGGNetModel, self).__init__()\n",
        "\n",
        "\n",
        "# https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/\n",
        "\n",
        "myModel = Flatten(name='flatten')(myModel) # specific names to layers, you can do so by name property\n",
        "myModel = Dense(512, activation='relu')(myModel)\n",
        "myModel = Dropout(0.3)(myModel) # regularization- to avoid overfitting \n",
        "myModel = Dense(len(lb.classes_), activation='softmax')(myModel)\n",
        "\n",
        "# what are the various ways to avoid overfitting?\n",
        "# 1) REGULARIZATION -> L1, L2, Dropout, Early Stopping (Median, Bandit, Truncation, Patience) [manual initially, but best automated]\n",
        "# 2) bias-variance tradeoff (adjust the values) [automated]\n",
        "# 3) hyperparamter tuning (adjust the values) [Grid Search, Random Search]\n",
        "# 4) Data Augmentation (word2Vectors, Noise, Image Augmentation, image overlays, filters, video mixers) (manual)\n",
        "# \n",
        "\n",
        "# Early stopping -> tf provides patience-> this is the continuous epochs for which a model will perform poorly \n",
        "# Better strategies-> RUN multiple algorithms in parallel, and early stop COMPARITIVELY poor performing algorithms\n",
        "#      1) MEDIAN early termination: FIND out the average performance of all algorithms, and terminate all below the \n",
        "#                 median performance (example: 30%, 40, 60, 62, 63). Here median is 60, so the first 2 algos are dropped\n",
        "#                 # YES, these are repeated and median keeps shifting \n",
        "#      2) TRUNCATION policy: tf patience style- based on absolute performance measure for a continous time period\n",
        "#                 example: threshold was 80%, and perfor: 30%, 40, 60, 62, 63, then all algos would be dropped!!!\n",
        "#      3) BANDIT policy-> over here the minimum performance threshold is set by us. All algos performing below it will\n",
        "#                 be terminated -> truncation's problem solved-> instead of absolute threshold, a dynamic threshold \n",
        "#                 is calculated. \n",
        "\n",
        "# most human effort should be focused on 3 and 4 \n",
        "# 1 and 2 can be automated, and there are many algorithms that will do it better than we can \n",
        "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters \n",
        "\n",
        "# HPs are Learning Rate, Epochs, Layers, hidden dimensions, number i can play around with and change model \n",
        "# performance \n",
        "# less or more data-> HP tuning is ALWAYS required \n",
        "# Hyperparameter tuning is painful task-> INFINITE POSSIBLE COMBINATIONS that could have been used\n",
        "# there is NO perfect answer in some cases! \n",
        "# Quest is to find either the BEST answer or a WORKABLE answer \n",
        "# if you want to find best answer, then the search-space for hyperparams has to be finite (SARIMAX=> (1,0,1,1)X(0,1,1))\n",
        "# Grid Search is to find best answer \n",
        "# it creates an EXHAUSTIVE truth table generating ALL combinations of Hyperparams, so just choose Min or Max \n",
        "\n",
        "# Random Search is to find workable, but not the best paramters- idea is to find something quickly \n",
        "# and move on -> it is done wherever Hyperparameter search space is INFINITE \n",
        "# Example: In most cases!!!! Grid search is exhaustive, time consuming and sometimes very expensive on big data\n",
        "# some grid search calculcations (enc/dec) can cross 1 BILLION years of computer/math time to calculate\n",
        "\n",
        "\n",
        "# in Resnet-> the mean values taken from ImageNet are global standards. So i as user will not be able to calculate\n",
        "# better means, because for that i will require all images on internet!!! \n",
        "# Global standards -> average for R,G and B are different (of course!)\n",
        "# if i change the positions of channels, than Blue channel could have got divided by mean from green channel which \n",
        "# would defeat the purpose of calculating mean from global data!!!!\n",
        "\n",
        "# in PandaVGG that we built-> we just mantained it via Channel Dimension = channels_first or _last\n",
        "# after that-> we calculated our averages per layers and didn't worry about whether it was RGB or BGR or any other format\n",
        "\n",
        "\n",
        "# model = basemodel + yourModel \n",
        "model = Model(inputs=basemodel.input, outputs=myModel) # connection of the two pipes -> ResNet and Mymodel \n",
        "\n",
        "for layer in basemodel.layers: # ResNet parameters should be preserved, and not learnt again\n",
        "  layer.trainable = False \n",
        "# Learning will be ONLY for myModel not for basemodel-> resnet weights will be preserved, while myModel will keep \n",
        "# learning \n",
        "# PARTIAL LEARNING-> some part of model is already smart, some part is learning from it's smartness \n",
        "\n",
        "# EVERYTHING is a GAME -> with outcomes, consequences and Players. \n",
        "# 2 kinds of games-> Zero Sum (win-win : clustering, encoding)\n",
        "# , non-zero sum (winners/losers- chess, classification, cross entropy, elections)\n",
        "\n",
        "# IF NOW YOU WANT MODEL TO BE FAMILIAR WITH YOUR IMAGES, you need to show images again!\n",
        "# but this brings us a problem- if i retain the model -> then the imagenet weights will be recalculated!\n",
        "\n",
        "# every time back-prop -> weights and bias are recalculated\n",
        "# everything that i inherited from resnet will GO AWAY except its shape!!\n",
        "# TO PRESERVE THE WEIGHTS INHERITED you can set that layers will NOT be learnt upon\n",
        "# thAT means their parameters will be unaffected by gradient descent \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wewYYqBdlljG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.summary()\n",
        "# Non trainable parameters-> non trainable ResNet which has layer.trainable = False \n",
        "# trainable params -> Dense layers towards the top, which is myModel "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3syxhtOl_h6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70aa2696-fc95-41cf-9144-2adeed835872"
      },
      "source": [
        "# what's the advantage of these non trainable parameters that we have inherited from Resnet/Imagenet \n",
        "# we are using Transfer Learning because these models already have been trained on MILLIONS of images\n",
        "# and already have weights that can help identify most of the objects \n",
        "# hence we can use them to generate weights for our problem and just manipulate them to the results what \n",
        "# we want. \n",
        "# if i retrain, then i lose information (w,b) on all previously trained images! \n",
        "# if i had to do that, i could have built a new model from scratch instead!!\n",
        "\n",
        "#Hyperparams\n",
        "HP_init_lr = 1e-4\n",
        "HP_epoch = 50\n",
        "HP_momentum = 0.9\n",
        "HP_batch_size = 32\n",
        "\n",
        "\n",
        "# MOMENTUM -> force + direction (vector gradient, not a scalar gradient descent )\n",
        "opt = SGD(lr=HP_init_lr, momentum = HP_momentum, decay= HP_init_lr//HP_epoch)\n",
        "\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit_generator(trainAug.flow(xtrain, ytrain, batch_size=HP_batch_size),\n",
        "                              steps_per_epoch = len(xtrain)//HP_batch_size,\n",
        "                              validation_data=valAug.flow(xtest, ytest),\n",
        "                              validation_steps = len(xtest)// HP_batch_size,\n",
        "                              epochs= HP_epoch)\n",
        "\n",
        "# CONVOLUTION layers even though are not fully connected, tend to perform better than \n",
        "# fully connected (Dense) layers, because of long sequences and knowing only partial info \n",
        "# of the patterns. This behavior can be even further enahnced by making sure that enough dropouts\n",
        "# and sufficiently long chains of conv2d are present \n",
        "# it's usually a good idea to compress or enlarge the pattern and start with SMALL patterns on \n",
        "# large data. THen in later layers, detect LARGE patterns in small (compressed) data. \n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "17/17 [==============================] - 172s 10s/step - loss: 1.6136 - acc: 0.2778 - val_loss: 1.6074 - val_acc: 0.2109\n",
            "Epoch 2/50\n",
            "17/17 [==============================] - 165s 10s/step - loss: 1.4543 - acc: 0.2791 - val_loss: 1.4755 - val_acc: 0.2613\n",
            "Epoch 3/50\n",
            "17/17 [==============================] - 166s 10s/step - loss: 1.3089 - acc: 0.4127 - val_loss: 1.2552 - val_acc: 0.3784\n",
            "Epoch 4/50\n",
            "17/17 [==============================] - 165s 10s/step - loss: 1.2125 - acc: 0.4174 - val_loss: 1.1585 - val_acc: 0.4865\n",
            "Epoch 5/50\n",
            "17/17 [==============================] - 164s 10s/step - loss: 1.0875 - acc: 0.5373 - val_loss: 1.0246 - val_acc: 0.5405\n",
            "Epoch 6/50\n",
            "17/17 [==============================] - 173s 10s/step - loss: 0.9964 - acc: 0.6011 - val_loss: 0.9338 - val_acc: 0.6328\n",
            "Epoch 7/50\n",
            "17/17 [==============================] - 162s 10s/step - loss: 0.9200 - acc: 0.6636 - val_loss: 0.8429 - val_acc: 0.6757\n",
            "Epoch 8/50\n",
            "17/17 [==============================] - 164s 10s/step - loss: 0.8954 - acc: 0.6544 - val_loss: 0.8114 - val_acc: 0.7297\n",
            "Epoch 9/50\n",
            "17/17 [==============================] - 164s 10s/step - loss: 0.7919 - acc: 0.7240 - val_loss: 0.7627 - val_acc: 0.7297\n",
            "Epoch 10/50\n",
            "17/17 [==============================] - 166s 10s/step - loss: 0.7248 - acc: 0.7426 - val_loss: 0.7308 - val_acc: 0.7387\n",
            "Epoch 11/50\n",
            "17/17 [==============================] - 167s 10s/step - loss: 0.7009 - acc: 0.7553 - val_loss: 0.6736 - val_acc: 0.7578\n",
            "Epoch 12/50\n",
            "17/17 [==============================] - 166s 10s/step - loss: 0.6967 - acc: 0.7794 - val_loss: 0.6648 - val_acc: 0.8018\n",
            "Epoch 13/50\n",
            "17/17 [==============================] - 163s 10s/step - loss: 0.6643 - acc: 0.7872 - val_loss: 0.4717 - val_acc: 0.8559\n",
            "Epoch 14/50\n",
            "17/17 [==============================] - 164s 10s/step - loss: 0.6213 - acc: 0.8151 - val_loss: 0.6468 - val_acc: 0.7658\n",
            "Epoch 15/50\n",
            "17/17 [==============================] - 166s 10s/step - loss: 0.5433 - acc: 0.8493 - val_loss: 0.5854 - val_acc: 0.8288\n",
            "Epoch 16/50\n",
            "17/17 [==============================] - 169s 10s/step - loss: 0.5106 - acc: 0.8534 - val_loss: 0.5446 - val_acc: 0.8359\n",
            "Epoch 17/50\n",
            "17/17 [==============================] - 163s 10s/step - loss: 0.5508 - acc: 0.8273 - val_loss: 0.4338 - val_acc: 0.9009\n",
            "Epoch 18/50\n",
            "17/17 [==============================] - 165s 10s/step - loss: 0.5196 - acc: 0.8323 - val_loss: 0.5127 - val_acc: 0.8468\n",
            "Epoch 19/50\n",
            "17/17 [==============================] - 161s 9s/step - loss: 0.5277 - acc: 0.8387 - val_loss: 0.5813 - val_acc: 0.8468\n",
            "Epoch 20/50\n",
            "17/17 [==============================] - 163s 10s/step - loss: 0.4737 - acc: 0.8593 - val_loss: 0.4371 - val_acc: 0.8649\n",
            "Epoch 21/50\n",
            "17/17 [==============================] - 169s 10s/step - loss: 0.4472 - acc: 0.8718 - val_loss: 0.4395 - val_acc: 0.8828\n",
            "Epoch 22/50\n",
            "17/17 [==============================] - 164s 10s/step - loss: 0.4656 - acc: 0.8736 - val_loss: 0.4820 - val_acc: 0.8649\n",
            "Epoch 23/50\n",
            "17/17 [==============================] - 175s 10s/step - loss: 0.4687 - acc: 0.8621 - val_loss: 0.3598 - val_acc: 0.8919\n",
            "Epoch 24/50\n",
            "17/17 [==============================] - 188s 11s/step - loss: 0.4294 - acc: 0.8889 - val_loss: 0.4620 - val_acc: 0.8559\n",
            "Epoch 25/50\n",
            "17/17 [==============================] - 189s 11s/step - loss: 0.4134 - acc: 0.8872 - val_loss: 0.4226 - val_acc: 0.8919\n",
            "Epoch 26/50\n",
            "17/17 [==============================] - 194s 11s/step - loss: 0.3966 - acc: 0.8897 - val_loss: 0.4130 - val_acc: 0.8906\n",
            "Epoch 27/50\n",
            "17/17 [==============================] - 187s 11s/step - loss: 0.4195 - acc: 0.8593 - val_loss: 0.3850 - val_acc: 0.9009\n",
            "Epoch 28/50\n",
            "17/17 [==============================] - 188s 11s/step - loss: 0.3825 - acc: 0.8960 - val_loss: 0.3979 - val_acc: 0.8919\n",
            "Epoch 29/50\n",
            "17/17 [==============================] - 191s 11s/step - loss: 0.4119 - acc: 0.8787 - val_loss: 0.3335 - val_acc: 0.9099\n",
            "Epoch 30/50\n",
            "17/17 [==============================] - 188s 11s/step - loss: 0.3612 - acc: 0.9043 - val_loss: 0.4019 - val_acc: 0.8829\n",
            "Epoch 31/50\n",
            "17/17 [==============================] - 192s 11s/step - loss: 0.3466 - acc: 0.9094 - val_loss: 0.3814 - val_acc: 0.8828\n",
            "Epoch 32/50\n",
            "17/17 [==============================] - 190s 11s/step - loss: 0.3679 - acc: 0.8934 - val_loss: 0.3510 - val_acc: 0.8919\n",
            "Epoch 33/50\n",
            "17/17 [==============================] - 185s 11s/step - loss: 0.3471 - acc: 0.9092 - val_loss: 0.3782 - val_acc: 0.9099\n",
            "Epoch 34/50\n",
            "17/17 [==============================] - 167s 10s/step - loss: 0.3393 - acc: 0.9108 - val_loss: 0.3133 - val_acc: 0.8829\n",
            "Epoch 35/50\n",
            "17/17 [==============================] - 167s 10s/step - loss: 0.3145 - acc: 0.9218 - val_loss: 0.3549 - val_acc: 0.9009\n",
            "Epoch 36/50\n",
            "17/17 [==============================] - 169s 10s/step - loss: 0.3232 - acc: 0.9083 - val_loss: 0.3469 - val_acc: 0.8984\n",
            "Epoch 37/50\n",
            "17/17 [==============================] - 162s 10s/step - loss: 0.3340 - acc: 0.9024 - val_loss: 0.3746 - val_acc: 0.8829\n",
            "Epoch 38/50\n",
            "17/17 [==============================] - 167s 10s/step - loss: 0.2965 - acc: 0.9224 - val_loss: 0.2627 - val_acc: 0.9369\n",
            "Epoch 39/50\n",
            "17/17 [==============================] - 183s 11s/step - loss: 0.2866 - acc: 0.9298 - val_loss: 0.3657 - val_acc: 0.8919\n",
            "Epoch 40/50\n",
            "17/17 [==============================] - 172s 10s/step - loss: 0.3157 - acc: 0.9022 - val_loss: 0.3071 - val_acc: 0.9009\n",
            "Epoch 41/50\n",
            "17/17 [==============================] - 169s 10s/step - loss: 0.2786 - acc: 0.9144 - val_loss: 0.3140 - val_acc: 0.8984\n",
            "Epoch 42/50\n",
            "17/17 [==============================] - 164s 10s/step - loss: 0.3016 - acc: 0.9089 - val_loss: 0.3150 - val_acc: 0.9189\n",
            "Epoch 43/50\n",
            "17/17 [==============================] - 168s 10s/step - loss: 0.2948 - acc: 0.9173 - val_loss: 0.3198 - val_acc: 0.9009\n",
            "Epoch 44/50\n",
            "17/17 [==============================] - 165s 10s/step - loss: 0.2916 - acc: 0.9071 - val_loss: 0.2805 - val_acc: 0.9189\n",
            "Epoch 45/50\n",
            "17/17 [==============================] - 164s 10s/step - loss: 0.2728 - acc: 0.9313 - val_loss: 0.3252 - val_acc: 0.9009\n",
            "Epoch 46/50\n",
            "17/17 [==============================] - 168s 10s/step - loss: 0.2724 - acc: 0.9251 - val_loss: 0.2872 - val_acc: 0.9141\n",
            "Epoch 47/50\n",
            "17/17 [==============================] - 166s 10s/step - loss: 0.2632 - acc: 0.9375 - val_loss: 0.3360 - val_acc: 0.9009\n",
            "Epoch 48/50\n",
            "17/17 [==============================] - 164s 10s/step - loss: 0.2926 - acc: 0.9077 - val_loss: 0.3018 - val_acc: 0.8919\n",
            "Epoch 49/50\n",
            "17/17 [==============================] - 164s 10s/step - loss: 0.2530 - acc: 0.9282 - val_loss: 0.2987 - val_acc: 0.9099\n",
            "Epoch 50/50\n",
            "17/17 [==============================] - 162s 10s/step - loss: 0.2948 - acc: 0.9051 - val_loss: 0.2508 - val_acc: 0.9279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUvKPPvho5Vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "4855943a-987e-4f2f-9d8d-dc519fef9650"
      },
      "source": [
        "predictions_binarized = model.predict(xtest, batch_size=HP_batch_size)\n",
        "# some metrics out of it \n",
        "# EXACTLY this way -> logo\n",
        "# dataset -> myLogo (1000 samples), otherLogo (2000)\n",
        "# EXACT-> image processing -> image_agumentation(logo) -> subset(main_image_augmentations)\n",
        "# for exact match, ML is a bad idea-> why? because cross entropy! we are okay with loss,\n",
        "# while for exact match, loss->0 \n",
        "\n",
        "# could've used ytest directly, but it is a matrix (binarized), classfication_report is built\n",
        "# only for flat values. [0 0 1] <- 3rd value is the correct class\n",
        "report = classification_report(ytest.argmax(axis=1), predictions_binarized.argmax(axis=1),\n",
        "                               target_names=lb.classes_) # inbuilt np.argmax -> gives index of max\n",
        "predictions = predictions_binarized.argmax(axis=1)\n",
        "# ALWAYS KEEP THIS while using LABEL BINARIZER\n",
        "\n",
        "print(report)\n",
        "\n",
        "# plot the history to diagonize performance\n",
        "N = HP_epoch\n",
        "plt.style.use('ggplot') # borrowing a style library \n",
        "plt.figure()\n",
        "plt.plot(np.arange(0,N), history.history['loss'], label='trainLoss')\n",
        "plt.plot(np.arange(0,N), history.history['val_loss'], label='ValidationLoss')\n",
        "plt.plot(np.arange(0,N), history.history['acc'], label='trainAcc')\n",
        "plt.plot(np.arange(0,N), history.history['val_acc'], label='valAcc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss&Acc')\n",
        "plt.legend(loc='lower left')\n",
        "plt.savefig(evaluation_path+'/history.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# this graphs shows an example of very good training, where more epoch and a larger dataset\n",
        "# could have taken the performance 95% +\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "   basketball       0.82      0.94      0.88        35\n",
            "       hockey       0.92      0.92      0.92        39\n",
            "     swimming       0.97      0.94      0.95        31\n",
            "weightlifting       0.94      0.84      0.89        38\n",
            "\n",
            "     accuracy                           0.91       143\n",
            "    macro avg       0.91      0.91      0.91       143\n",
            " weighted avg       0.91      0.91      0.91       143\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEJCAYAAAB7UTvrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJcpNJQZIkSXL7f1vHZWBji7wiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNwQqJGkN2gm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(model_path + '/model.h5')\n",
        "# save the binarized labels\n",
        "f = open(model_path+'/labels.pkl', 'wb')\n",
        "f.write(pickle.dumps(lb))\n",
        "f.close()\n",
        "# model and labels saved for future use \n",
        "# train.py ends here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGJu5-4EOm9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction of activities \n",
        "# Score.py begins here\n",
        "\n",
        "from keras.models import load_model\n",
        "from collections import deque \n",
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3ZEidO6RYuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UI_model_path = 'bin/model.h5'\n",
        "UI_labels = 'bin/labels.pkl'\n",
        "UI_input_video = 'test/example_activities.mp4'\n",
        "UI_activity_frame_length = 128 # come back and change this number\n",
        "# we are assuming an activity lasts for 32 frames on the video to be detected as a pattern \n",
        "# Speed = distance/ time \n",
        "# FPS = Frames / video_length \n",
        "# distance = speed X time \n",
        "# 20 FPS = 20 frames in 1 second, for a video length of 1 min= 20 X 60 = 120 frames \n",
        "\n",
        "# ROLLING WINDOW on our video and detect activities inside that window\n",
        "# Detect activity for each frame inside the rolling window, and using a statistical method\n",
        "# can decide average or majority or minority activity "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmJBfGxNSHEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model(UI_model_path)\n",
        "lb = pickle.loads(open(UI_labels,'rb').read())\n",
        "UI_activity_frame_length = 128\n",
        "# QUEUE of frames and detect pattern a rolling-window on it? \n",
        "# average pixel values from ImageNet\n",
        "mean = np.array( [ 103.939, 116.779, 123.68 ]).astype('float32')\n",
        "q = deque(maxlen=UI_activity_frame_length)\n",
        "# dynamic queue, a type of data structure\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB8Gm5nZU44-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# consume the video and break into frames \n",
        "from google.colab.patches import cv2_imshow\n",
        "vs = cv2.VideoCapture('test/180301_01_B_Basketball_13.mp4')\n",
        "# initialize an empty writer_pointer and empty frame, to write output frames\n",
        "writer = None\n",
        "(W,H) = (None, None)\n",
        "\n",
        "while True:\n",
        "  # read a frame\n",
        "  (grabbed, frame) = vs.read()\n",
        "  if not grabbed: # End-of-line or \\0 or NULL or images \n",
        "    break\n",
        "  # if frame is empty, intialize frame, if already initialized, continue with that frame\n",
        "  if W is None or H is None:\n",
        "    (H,W) = frame.shape[:2] # gives first 2 elements which are arranged in height, width format\n",
        "  output = frame.copy()\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  frame = cv2.resize(frame, (224,224)).astype('float32')\n",
        "  # calculate mean error from the ImageNet means\n",
        "  frame -= mean \n",
        "  corrected_frame = np.expand_dims(frame, axis=0)\n",
        "  # a healthy, processed frame ready for my model \n",
        "  preds = model.predict(corrected_frame)  # this will be a LIST of binarized items, inside another list \n",
        "  img_prediction = preds[0]\n",
        "  q.append(img_prediction) \n",
        "  # added data to rolling window, and calculate new rolled-average \n",
        "  results = np.array(q).mean(axis=0)\n",
        "  i = np.argmax(results)\n",
        "  label = lb.classes_[i]\n",
        "  # label name achieved \n",
        "  \n",
        "  # let's update our empty frame!!!\n",
        "  # the axis images follow-> left to right-> positive, top to bottom-> positive\n",
        "  # right to left /  bottom to top -> neg\n",
        "  text = label + \" Detected!\"\n",
        "  cv2.putText(output, text, (35,35), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (255,0,0), 5)\n",
        "  # 35,35 is text to left,top\n",
        "  if writer is None:\n",
        "    # fourcc is the encoding 4 bytes used by MPEG videos \n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")  # building encoding\n",
        "    writer = cv2.VideoWriter('bin/output_b.avi', fourcc, 30, (W,H))\n",
        "  writer.write(output)\n",
        "  #cv2.imshow(output)\n",
        "\n",
        "  # take a user input to exit, lets say x to exit \n",
        "  key = cv2.waitKey(1) & 0xFF # correct way of taking user input from openCV \n",
        "  \n",
        "  if(key == ord('x')): # converts the cv2 output to the respective integer, all chars are numbers\n",
        "    break\n",
        "\n",
        "\n",
        "writer.release()\n",
        "vs.release()\n",
        "# ALL SOUND WILL BE LOST IN THE RESULTING VIDEO\n",
        "# IF original video had any sound, it should be overlayed\n",
        "# separately \n",
        "# this is possible as long as there is no compression/decom\n",
        "# on image size or Frames-per-second\n",
        "# otherwise sound and video will desync! \n",
        "# TOTAL PLAYING TIME of audio and video cannot change for overlay\n",
        "# to happen without data loss \n",
        "\n",
        "\n",
        "\n",
        "  # make sure you do this whenever taking user input from OPEN CV-> camera\n",
        "  # for portability, make sure that this works for both 16 bit or above it\n",
        "  # if i had to represent, 1, then 01, 00 01, 0000 0001 are all same-> their lowest signifact\n",
        "  # byte is 1 hence i could have just looked at the last number \n",
        "  # for 16 bit, i can take lower two bytes of data! \n",
        "  # can be achieved through AND operation with 1111 1111 !\n",
        "  # 0101 0101 1010 1010 (large data piece!)\n",
        "  #     0101 0101 1010 1010  \n",
        "  # &             1111 1111\n",
        "  # r=  0000 0000 1010 1010   <- last 1 byte got saved! \n",
        "  # to save 16 bits, filter should be 1111 1111 1111 1111!\n",
        "  # in hexadecimal, we can refer to this as F F \n",
        "  # hex(0xF)  = 1111 1111\n",
        "  # hex(0xFF) = 1111 1111 1111 1111\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Since this is a video, it will come to an end \n",
        "  # if this was a live feed or a webcam, then we should also have a way to exit, because this\n",
        "  # will become an infinite loop \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbLvqfL7cBoc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd3dee18-b7ee-4e4d-98fb-721d7ef54c01"
      },
      "source": [
        "# link to previous tests\n",
        "# https://1drv.ms/u/s!AhM-uOEWdqAexFdgz8rD6bB5LNMQ?e=UrYZDi \n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2a (Conv2D)         (None, 56, 56, 64)   4160        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch1 (Conv2D)          (None, 56, 56, 256)  16640       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch1 (BatchNormalizatio (None, 56, 56, 256)  1024        res2a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 56, 56, 256)  0           bn2a_branch2c[0][0]              \n",
            "                                                                 bn2a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 56, 56, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 56, 56, 256)  0           bn2b_branch2c[0][0]              \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 56, 56, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 56, 56, 256)  0           bn2c_branch2c[0][0]              \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 56, 56, 256)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2a (Conv2D)         (None, 28, 28, 128)  32896       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch1 (Conv2D)          (None, 28, 28, 512)  131584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch1 (BatchNormalizatio (None, 28, 28, 512)  2048        res3a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 28, 28, 512)  0           bn3a_branch2c[0][0]              \n",
            "                                                                 bn3a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 28, 28, 512)  0           bn3b_branch2c[0][0]              \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 28, 28, 512)  0           bn3c_branch2c[0][0]              \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 28, 28, 512)  0           bn3d_branch2c[0][0]              \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 28, 28, 512)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n",
            "                                                                 bn4a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n",
            "                                                                 activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 14, 14, 1024) 0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
            "                                                                 bn5a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
            "                                                                 activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 7, 7, 2048)   0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 2048)   0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2048)         0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          1049088     flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 4)            2052        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 24,638,852\n",
            "Trainable params: 1,051,140\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bd84K4HNDFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data can be downloaded from\n",
        "# https://1drv.ms/u/s!AhM-uOEWdqAesjQePSPc7XqEp7ox?e=bUsSuS"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}