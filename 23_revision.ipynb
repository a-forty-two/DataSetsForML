{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "23 revision.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTSP0iSn9KczGqmEsF41wp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/DataSetsForML/blob/master/23_revision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8XDlphGfB1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SAGEMAKER -> import data from S3 \n",
        "import boto3\n",
        "import pandas as pd\n",
        "from sagemaker import get_execution_role\n",
        "\n",
        "role = get_execution_role()\n",
        "bucket='my-bucket'\n",
        "data_key = 'train.csv'\n",
        "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
        "\n",
        "pd.read_csv(data_location)\n",
        "\n",
        "# Autocorrelation <- corrleation with oneself <- a variable correlated with itself is always 1!\n",
        "# Autocorrelation steps the data on time axis! as a result you have latency between data \n",
        "# and itself. Then autocorrelation can be used to measure deviations from original data. \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCnY_nkKfMns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spark -> MLLib\n",
        "https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a\n",
        "\n",
        "# Markov Chains -> sequential event probabilities "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfnx8gaHnsBv",
        "colab_type": "text"
      },
      "source": [
        "Computer Science Fundamentals - a quick review \n",
        "\n",
        "\n",
        "Computing is a branch under ELECTRICAL ENGINEERING. All concepts, formulae and derivation are fundamentally in EE. \n",
        "\n",
        "ON and OFF -> Light Bulb \n",
        "1      0\n",
        "\n",
        "ELECTRICAL SIGNALS COULD BE READ/WRITE at high speeds\n",
        "\n",
        "This lead to simple electrical circuits called GATES \n",
        "\n",
        "GATE: AND, OR, XOR, NOT \n",
        "\n",
        "https://whatis.techtarget.com/definition/logic-gate-AND-OR-XOR-NOT-NAND-NOR-and-XNOR\n",
        "\n",
        "\n",
        "Gates combinations became more and more complex, around same time, semiconductors were discovered \n",
        "\n",
        "Silicon -> high conductivity along with being able to change its conductivity state, allowed more state representations with help of SIMPLE gates becoming more and more complex. \n",
        "\n",
        "Gates -> from simple electrical circuits, they became SEMICONDUCTOR based circuits \n",
        "\n",
        "Adder/Counter, Register new types of gates!\n",
        "\n",
        "CPU = Control Unit (Gates) + Memory Unit (more gates) + Arithmetic-Logic Unit (even more gates)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSQSn0dpptf6",
        "colab_type": "text"
      },
      "source": [
        "The game changers: INTEL, nVidia, AMD, Qualcomm \n",
        "\n",
        "x86, x64 etc- different architectures of hardware- primarily registers (storage and computation gates), RAM (computation gates), ROM (read only gates)..\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa5iTIvCqa_E",
        "colab_type": "text"
      },
      "source": [
        "A Microprocessor (pronounced mu-P mu is 1e-6) consists of a lot of PINS to collect input and provide output from/to other electrical devices. \n",
        "\n",
        "Any external device which has a wire output or input, can be connected to one of the PINS. these pings are called GPIO -> general purpose Input Output pins. \n",
        "\n",
        "They contain storage and calculation units called Registers. There are 6 common types of registers (more types exist):\n",
        "\n",
        "1) Accumulator -> CALCULATE + operation (sub-> addition in negative direction,multiplication is also addition! div is also addition (sub again and again) -> with help of just 1 operation, Accumulator can calculate ANYTHING. Operations happen ONE at a TIME. \n",
        "\n",
        "2) Basic register -> usually for temporary storage \n",
        "\n",
        "3) Counter register -> keeps a count of things, used for LOOPs, recursion, keeping counts \n",
        "\n",
        "4) DX register -> mainly for printing and user input \n",
        "\n",
        "5 and 6) SI and DI -> Stack Index and Data Index (POINTERS)\n",
        "\n",
        "\n",
        "REGISTERS are hardware. Than means, once built cannot be changed!!! \n",
        "\n",
        "They should be hold all kinds of data that exists in a computer. \n",
        "\n",
        "This is a problem! sizeof(int) != sizeof(double) \n",
        "\n",
        "\"Hello world\" is not of same size as 42. \n",
        "\n",
        "How do i create a storage unit that would have been able to store both types of data??\n",
        "\n",
        "Solution-> Don't store data in register. Store ONLY the address of data in register. BEcause doesn't matter  what type of data is, ADDRESS IS ALWAYS OF THE SAME SIZE!!\n",
        "\n",
        "house no, block no., road no., ZIP... cou <- your address structure-> EVERYBODY COULD FIT UNIFORMLY. \n",
        "\n",
        "THIS IS CALLED A POINTER. Pointer stores the address of starting of data. While rest of the data is actually store somewhere else where space is available in bulk (RAM!) \n",
        "\n",
        "Register collects/writes data from/to RAM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC3428aTsm2F",
        "colab_type": "text"
      },
      "source": [
        "ALL this is ELECTRICITY,whose language is 0 and 1. OFF and ON.\n",
        "\n",
        "muPs speak binary. \n",
        "\n",
        "Add 1,2 \n",
        "\n",
        "add                        1           2\n",
        "\n",
        "0110...01   0000 0001   0000 0010\n",
        "\n",
        "This makes these operations RUNNING in a single thread. This sequence of steps to execute requires dedicated hardware -> THREAD (commands + hardware combination). \n",
        "0110...01   0000 0001   0000 0010\n",
        "0110...01   0000 0001   1101 1010\n",
        "0110...01   0000 0001   0000 0010\n",
        "\n",
        "\n",
        "THIS MEANS that unless we do parallelization there will be no increase in speed. \n",
        "\n",
        "MULTI-CORE systems -> but they also hit speed threshold beyond which no performance improvement \n",
        "\n",
        "\n",
        "GPU -> I will do calculations slowly, but MASSIVE amounts of calculations at a time. As a result, you get more operations solved in a fixed given time than many CPUs working together! \n",
        "\n",
        "GPU is slower than CPU!!! but higher throughput \n",
        "\n",
        "GPU sizes are in Gigabytes (size of data that can be processed in parallel), not in speed of calculation (like GHz or MHz in processors) \n",
        "\n",
        "GPU -> fast graph calculations, primarily int/diff. Slow math calculations. \n",
        "\n",
        "CPU -> fast math calculations, poor performance as the size of operations increase \n",
        "\n",
        "GPUs contain a LOT more gates than CPUs thus making parallel computing butter smooth. \n",
        "\n",
        "\n",
        "Reminder: less than  100kb of RAM had put us on moon.\n",
        "\n",
        "Most of your AI actually doesn't require GBs of RAM. THis means, it could have worked DIRECTLY on CPUs or GPUs if it was small enough. \n",
        "\n",
        "2 main devices which implement microP and microCs into workable AI devices:\n",
        "\n",
        "1) Raspberry PI (micro Processor) \n",
        "\n",
        "2) Arduino (micro Controller)\n",
        "\n",
        "Processor -> computation unit. Cannot do anything else. Requires other devices to become a complete computer. (Docker, Kubernete for IoT will be deployed on a cluster of processors or GPUs)\n",
        "\n",
        "Controller -> requires a processor to compute, but is a complete machine by itself otherwise. \n",
        "(ROBOTICs, automation, self-driven cars, rotor motors-> controllers) \n",
        "\n",
        "https://www.academia.edu/31861833/Microprocessors_-_Douglas_V._Hall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-dqn1K4xXqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test prep\n",
        "# regularization -> what is it fixing? OVERFITTING! \n",
        "# Avoid underfitting -> use more data \n",
        "\n",
        "# Stages of machine learning \n",
        "\n",
        "# 1) Collection of data -> IoT, Web applications, databases, docs, files, images, etc. \n",
        "# 2) STORE the data as it is -> Data Lake, HDFS, HBase \n",
        "# 3) Read the data and begin cleaning-> PANDAS-> creates panel dataframes out of data points \n",
        "# these dataframes are then checked for null values, missing data, illogical data etc. \n",
        "# CLEAN data should always be stored in STRUCTURED format. Any unstructured data SHOULD be \n",
        "# avoided for ML as all Neural Networks are hardcoded \n",
        "# 4) Structured data should be stored, and steps to clean recorded to avoid repeating the steps\n",
        "# on same data again. And reproduce steps for new data points. \n",
        "# Typically, good practice is to create a script file out of it. \n",
        "# 5) Exploratory data analysis -> MATPLOTLIB, SEABORN, normalization, Correlations,and so on \n",
        "# Data is visualized to observe the trends, directions of data points, OUTLIERS! \n",
        "# z-score/t-score happen at this stage \n",
        "# 6) decide business objectives-> what to infer? are we trying to classify?regress?cluster?\n",
        "# identify your Business Objective as Y, and rest of the data is X\n",
        "# now business objective is Y = f(X) \n",
        "# just representation, not actual solution. This f(X) still needs to be calculated. \n",
        "# one way of calculating was that manually sit with data and keep guessing (educated guesses),\n",
        "# each time slowly coming to right answer. Or apply Mathematical Induction!\n",
        "# but that can take 100s of years!\n",
        "# 7) either ML or DL: instead of induction and guessing and mistake-correction manually, we \n",
        "# automate it either through Graph theory (graph-based algos) or Pattern recognition (neurals)\n",
        "# 8) Apply multiple algorithms (or with diff hyperparams). Each of this combination is called \n",
        "# a 'RUN'. Within a run, each algorithm is going to run its EPOCHS. \n",
        "# Early stopping should be used over here, along with either GRID or RANDOM search to arrive\n",
        "# at best algorithm with best hyperparams. SHORTLIST at least 3-5 such algorithms based on \n",
        "# metrics and evaluation. \n",
        "# 9) Put all algos into staging. Observe for a period of time with reinforcement (new data\n",
        "# is appended to old data) and equal hardware. \n",
        "# 10) select best surviving algo over a period of time. \n",
        "# 11) EXPORT shape and weights of the model and save them as files to be used later \n",
        "\n",
        "# ALWAYS MANTAIN THE DIRECTORY STRUCTURE \n",
        "\n",
        "# HYPERPARAMS are MANUALLY SET, and NOT provided by the data-owner.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNomXJCk1LRU",
        "colab_type": "text"
      },
      "source": [
        "Cheat Sheet to using layers:\n",
        "\n",
        "PATTERN RECOGNITION\n",
        "\n",
        "DENSE + DENSE -> two fully connected layers generate weights that can be used to detect patterns \n",
        "\n",
        "regression problem? Minimum 3 layers are required!\n",
        "\n",
        "Input -> Hidden -> output\n",
        "\n",
        "Input -> Dense -> Dense (Shallow learning)\n",
        "\n",
        "Dense -> dense -> dense (beginning of deep learning)\n",
        "\n",
        "Input in this case could have 2 layers:\n",
        "\n",
        "1) Input Layer- doesn't do anything, just takes the input and passes it forward. Typically used to complete a network, or when a layer cannot be as input layer (ex. Conv2D)\n",
        "\n",
        "2) Dense Layer - deeper learning, first patterns will be generated (features), then even finer patterns on patterns. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "When working with images, always FLATTEN them out. \n",
        "\n",
        "FLATTEN layer take all input and reshapes into single dimension data.\n",
        "\n",
        "When working with text, always ENCODE. \n",
        "Encoding is manual. Neurals are taught encoding with help of EMBEDDING layer. \n",
        "\n",
        "Embedding layers learns the vectors of an input based on the dictionary you provide. \n",
        "\n",
        "Input of embedding layer is a MATRIX-> rows represent each DICTIONARY WORD, columns represent the input sentence. With help of 1-hot matrices, and previously defined ranks, EMbedding layer converts simple numbers into VECTORS. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O0niUBJ3hXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Embedding(VOCAB_SIZE, 16, input_length=sentence_size)\n",
        "# the above line would create a matrix, with VOCAB_SIZE rows, and sentence_size columns\n",
        "\n",
        "# ranks: I:33, Going:13, am:5, home:24, today:44\n",
        "# step 1 of embedding layer \n",
        "#        <PAD>  <PAD> <PAD>  today  i   home \n",
        "# i          0   0     0      0     1    0\n",
        "#  going     0   0     0      0     0    0\n",
        "#  am        0   0     0      0     0    0\n",
        "#  home      0   0     0      0     0    1\n",
        "#  today     0   0     0      1     0    0\n",
        "# step 2: multiply this matrix with the weights or ranks of the words of sentence \n",
        "#        <PAD>  <PAD> <PAD>  today  i   home                \n",
        "# i          0   0     0      0     1    0                          33\n",
        "#  going     0   0     0      0     0    0          X Ranks =        0\n",
        "#  am        0   0     0      0     0    0                           0 \n",
        "#  home      0   0     0      0     0    1                          24\n",
        "#  today     0   0     0      1     0    0                          44\n",
        "\n",
        "# the sequence generate at the end, is broken into OUTPUT_DIMENSIONS and \n",
        "# passed as features to next layer\n",
        "# these broken OUTPUT_DIMENSIONS are now called VECTORS. "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}