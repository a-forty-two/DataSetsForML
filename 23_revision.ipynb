{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "23 revision.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDFXjrInzcPQ3W4aNvcqMq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/DataSetsForML/blob/master/23_revision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8XDlphGfB1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SAGEMAKER -> import data from S3 \n",
        "import boto3\n",
        "import pandas as pd\n",
        "from sagemaker import get_execution_role\n",
        "\n",
        "role = get_execution_role()\n",
        "bucket='my-bucket'\n",
        "data_key = 'train.csv'\n",
        "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
        "\n",
        "pd.read_csv(data_location)\n",
        "\n",
        "# Autocorrelation <- corrleation with oneself <- a variable correlated with itself is always 1!\n",
        "# Autocorrelation steps the data on time axis! as a result you have latency between data \n",
        "# and itself. Then autocorrelation can be used to measure deviations from original data. \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCnY_nkKfMns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spark -> MLLib\n",
        "https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a\n",
        "\n",
        "# Markov Chains -> sequential event probabilities "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfnx8gaHnsBv",
        "colab_type": "text"
      },
      "source": [
        "Computer Science Fundamentals - a quick review \n",
        "\n",
        "\n",
        "Computing is a branch under ELECTRICAL ENGINEERING. All concepts, formulae and derivation are fundamentally in EE. \n",
        "\n",
        "ON and OFF -> Light Bulb \n",
        "1      0\n",
        "\n",
        "ELECTRICAL SIGNALS COULD BE READ/WRITE at high speeds\n",
        "\n",
        "This lead to simple electrical circuits called GATES \n",
        "\n",
        "GATE: AND, OR, XOR, NOT \n",
        "\n",
        "https://whatis.techtarget.com/definition/logic-gate-AND-OR-XOR-NOT-NAND-NOR-and-XNOR\n",
        "\n",
        "\n",
        "Gates combinations became more and more complex, around same time, semiconductors were discovered \n",
        "\n",
        "Silicon -> high conductivity along with being able to change its conductivity state, allowed more state representations with help of SIMPLE gates becoming more and more complex. \n",
        "\n",
        "Gates -> from simple electrical circuits, they became SEMICONDUCTOR based circuits \n",
        "\n",
        "Adder/Counter, Register new types of gates!\n",
        "\n",
        "CPU = Control Unit (Gates) + Memory Unit (more gates) + Arithmetic-Logic Unit (even more gates)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSQSn0dpptf6",
        "colab_type": "text"
      },
      "source": [
        "The game changers: INTEL, nVidia, AMD, Qualcomm \n",
        "\n",
        "x86, x64 etc- different architectures of hardware- primarily registers (storage and computation gates), RAM (computation gates), ROM (read only gates)..\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa5iTIvCqa_E",
        "colab_type": "text"
      },
      "source": [
        "A Microprocessor (pronounced mu-P mu is 1e-6) consists of a lot of PINS to collect input and provide output from/to other electrical devices. \n",
        "\n",
        "Any external device which has a wire output or input, can be connected to one of the PINS. these pings are called GPIO -> general purpose Input Output pins. \n",
        "\n",
        "They contain storage and calculation units called Registers. There are 6 common types of registers (more types exist):\n",
        "\n",
        "1) Accumulator -> CALCULATE + operation (sub-> addition in negative direction,multiplication is also addition! div is also addition (sub again and again) -> with help of just 1 operation, Accumulator can calculate ANYTHING. Operations happen ONE at a TIME. \n",
        "\n",
        "2) Basic register -> usually for temporary storage \n",
        "\n",
        "3) Counter register -> keeps a count of things, used for LOOPs, recursion, keeping counts \n",
        "\n",
        "4) DX register -> mainly for printing and user input \n",
        "\n",
        "5 and 6) SI and DI -> Stack Index and Data Index (POINTERS)\n",
        "\n",
        "\n",
        "REGISTERS are hardware. Than means, once built cannot be changed!!! \n",
        "\n",
        "They should be hold all kinds of data that exists in a computer. \n",
        "\n",
        "This is a problem! sizeof(int) != sizeof(double) \n",
        "\n",
        "\"Hello world\" is not of same size as 42. \n",
        "\n",
        "How do i create a storage unit that would have been able to store both types of data??\n",
        "\n",
        "Solution-> Don't store data in register. Store ONLY the address of data in register. BEcause doesn't matter  what type of data is, ADDRESS IS ALWAYS OF THE SAME SIZE!!\n",
        "\n",
        "house no, block no., road no., ZIP... cou <- your address structure-> EVERYBODY COULD FIT UNIFORMLY. \n",
        "\n",
        "THIS IS CALLED A POINTER. Pointer stores the address of starting of data. While rest of the data is actually store somewhere else where space is available in bulk (RAM!) \n",
        "\n",
        "Register collects/writes data from/to RAM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC3428aTsm2F",
        "colab_type": "text"
      },
      "source": [
        "ALL this is ELECTRICITY,whose language is 0 and 1. OFF and ON.\n",
        "\n",
        "muPs speak binary. \n",
        "\n",
        "Add 1,2 \n",
        "\n",
        "add                        1           2\n",
        "\n",
        "0110...01   0000 0001   0000 0010\n",
        "\n",
        "This makes these operations RUNNING in a single thread. This sequence of steps to execute requires dedicated hardware -> THREAD (commands + hardware combination). \n",
        "0110...01   0000 0001   0000 0010\n",
        "0110...01   0000 0001   1101 1010\n",
        "0110...01   0000 0001   0000 0010\n",
        "\n",
        "\n",
        "THIS MEANS that unless we do parallelization there will be no increase in speed. \n",
        "\n",
        "MULTI-CORE systems -> but they also hit speed threshold beyond which no performance improvement \n",
        "\n",
        "\n",
        "GPU -> I will do calculations slowly, but MASSIVE amounts of calculations at a time. As a result, you get more operations solved in a fixed given time than many CPUs working together! \n",
        "\n",
        "GPU is slower than CPU!!! but higher throughput \n",
        "\n",
        "GPU sizes are in Gigabytes (size of data that can be processed in parallel), not in speed of calculation (like GHz or MHz in processors) \n",
        "\n",
        "GPU -> fast graph calculations, primarily int/diff. Slow math calculations. \n",
        "\n",
        "CPU -> fast math calculations, poor performance as the size of operations increase \n",
        "\n",
        "GPUs contain a LOT more gates than CPUs thus making parallel computing butter smooth. \n",
        "\n",
        "\n",
        "Reminder: less than  100kb of RAM had put us on moon.\n",
        "\n",
        "Most of your AI actually doesn't require GBs of RAM. THis means, it could have worked DIRECTLY on CPUs or GPUs if it was small enough. \n",
        "\n",
        "2 main devices which implement microP and microCs into workable AI devices:\n",
        "\n",
        "1) Raspberry PI (micro Processor) \n",
        "\n",
        "2) Arduino (micro Controller)\n",
        "\n",
        "Processor -> computation unit. Cannot do anything else. Requires other devices to become a complete computer. (Docker, Kubernete for IoT will be deployed on a cluster of processors or GPUs)\n",
        "\n",
        "Controller -> requires a processor to compute, but is a complete machine by itself otherwise. \n",
        "(ROBOTICs, automation, self-driven cars, rotor motors-> controllers) \n",
        "\n",
        "https://www.academia.edu/31861833/Microprocessors_-_Douglas_V._Hall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-dqn1K4xXqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test prep\n",
        "# regularization -> what is it fixing? OVERFITTING! \n",
        "# Avoid underfitting -> use more data \n",
        "\n",
        "# Stages of machine learning \n",
        "\n",
        "# 1) Collection of data -> IoT, Web applications, databases, docs, files, images, etc. \n",
        "# 2) STORE the data as it is -> Data Lake, HDFS, HBase \n",
        "# 3) Read the data and begin cleaning-> PANDAS-> creates panel dataframes out of data points \n",
        "# these dataframes are then checked for null values, missing data, illogical data etc. \n",
        "# CLEAN data should always be stored in STRUCTURED format. Any unstructured data SHOULD be \n",
        "# avoided for ML as all Neural Networks are hardcoded \n",
        "# 4) Structured data should be stored, and steps to clean recorded to avoid repeating the steps\n",
        "# on same data again. And reproduce steps for new data points. \n",
        "# Typically, good practice is to create a script file out of it. \n",
        "# 5) Exploratory data analysis -> MATPLOTLIB, SEABORN, normalization, Correlations,and so on \n",
        "# Data is visualized to observe the trends, directions of data points, OUTLIERS! \n",
        "# z-score/t-score happen at this stage \n",
        "# 6) decide business objectives-> what to infer? are we trying to classify?regress?cluster?\n",
        "# identify your Business Objective as Y, and rest of the data is X\n",
        "# now business objective is Y = f(X) \n",
        "# just representation, not actual solution. This f(X) still needs to be calculated. \n",
        "# one way of calculating was that manually sit with data and keep guessing (educated guesses),\n",
        "# each time slowly coming to right answer. Or apply Mathematical Induction!\n",
        "# but that can take 100s of years!\n",
        "# 7) either ML or DL: instead of induction and guessing and mistake-correction manually, we \n",
        "# automate it either through Graph theory (graph-based algos) or Pattern recognition (neurals)\n",
        "# 8) Apply multiple algorithms (or with diff hyperparams). Each of this combination is called \n",
        "# a 'RUN'. Within a run, each algorithm is going to run its EPOCHS. \n",
        "# Early stopping should be used over here, along with either GRID or RANDOM search to arrive\n",
        "# at best algorithm with best hyperparams. SHORTLIST at least 3-5 such algorithms based on \n",
        "# metrics and evaluation. \n",
        "# 9) Put all algos into staging. Observe for a period of time with reinforcement (new data\n",
        "# is appended to old data) and equal hardware. \n",
        "# 10) select best surviving algo over a period of time. \n",
        "# 11) EXPORT shape and weights of the model and save them as files to be used later \n",
        "\n",
        "# ALWAYS MANTAIN THE DIRECTORY STRUCTURE \n",
        "\n",
        "# HYPERPARAMS are MANUALLY SET, and NOT provided by the data-owner.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNomXJCk1LRU",
        "colab_type": "text"
      },
      "source": [
        "Cheat Sheet to using layers:\n",
        "\n",
        "PATTERN RECOGNITION\n",
        "\n",
        "DENSE + DENSE -> two fully connected layers generate weights that can be used to detect patterns \n",
        "\n",
        "regression problem? Minimum 3 layers are required!\n",
        "\n",
        "Input -> Hidden -> output\n",
        "\n",
        "Input -> Dense -> Dense (Shallow learning)\n",
        "\n",
        "Dense -> dense -> dense (beginning of deep learning)\n",
        "\n",
        "Input in this case could have 2 layers:\n",
        "\n",
        "1) Input Layer- doesn't do anything, just takes the input and passes it forward. Typically used to complete a network, or when a layer cannot be as input layer (ex. Conv2D)\n",
        "\n",
        "2) Dense Layer - deeper learning, first patterns will be generated (features), then even finer patterns on patterns. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "When working with images, always FLATTEN them out. \n",
        "\n",
        "FLATTEN layer take all input and reshapes into single dimension data.\n",
        "\n",
        "When working with text, always ENCODE. \n",
        "Encoding is manual. Neurals are taught encoding with help of EMBEDDING layer. \n",
        "\n",
        "Embedding layers learns the vectors of an input based on the dictionary you provide. \n",
        "\n",
        "Input of embedding layer is a MATRIX-> rows represent each DICTIONARY WORD, columns represent the input sentence. With help of 1-hot matrices, and previously defined ranks, EMbedding layer converts simple numbers into VECTORS. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti4CszZQWMGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# No. of parameters (Dense layer) =>      input_dimension X output_dimension + bias \n",
        "# what's the value of bias? 1 per output_dimension \n",
        "# input_dimension X output_dimension + output_dimension \n",
        "\n",
        "# No. of parameters (Flattening Layer) => Flattening layer's job is to reshape the data.\n",
        "# for this, it does not need ANY new information! hence no PARAMETERS are learnt!\n",
        "\n",
        "# No. of parameters (Dropout) => This layer's job is to randomly select N number of dimensions\n",
        "# and multiply them with 0. This selection is different every epoch. \n",
        "# NO NEW PARAMETERS REQUIRED AS THIS IS JUST A RANDOM PROCESS!!! \n",
        "\n",
        "# No. of paramters (Convolutions)=> \n",
        "# ((filter_height_shape X filter_width_shape) +b)X no. of filters \n",
        "\n",
        "\n",
        "\n",
        "# for this, it does not need ANY new information! hence no PARAMETERS are learnt!\n",
        "\n",
        "# OPTIMIZERS -> help loss functions reduce their value with help of descend and optionally,\n",
        "# momentum \n",
        "# SGD, Adam, RMSProp \n",
        "# RMSProp -> root mean squared propagation (better Optimizer for Regression problems)\n",
        "# RMSProp with Momentum (direction-push towards local minima) is called Adam Optimizer \n",
        "# Adam optimizer is great for both Regression and Classification problems \n",
        "# https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
        "\n",
        "# Adam = RMSProp with momentum -> momentum is PUSH towards the answer's direction \n",
        "\n",
        "# Not just the magnitude but also the direction is chosen for gradient descend \n",
        "\n",
        "\n",
        "#     Convolutional Filters -> MATRICES that get multiplied by input \n",
        "#     Convolutional algos-> take original input and draw sliding windows on it. \n",
        "#     Each sliding window is multiplied with a CONVOLUTION. \n",
        "#     Result is called an ACTIVATION MAP\n",
        "#     Activation Map serves as feature(input) for the next convolution \n",
        "#     It's usually normalized before being passed as input \n",
        "\n",
        "#    1 Dimensional Convolutions \n",
        "#     Data->         0011010101010101010111100101\n",
        "#    Pattern -> 0011 -> 1 detecions\n",
        "#               10   -> 10 detections\n",
        "#               11   -> 4 sliding window detections\n",
        "\n",
        "#      LARGE PAttern-> small detections\n",
        "# SMALL PATTERNS -> large detecions\n",
        "\n",
        "# I've an image (assume grayscale- 1 channel) -> 2-D Data of pixels\n",
        "# If i wanted to detect i diagonal \n",
        "# \n",
        "\n",
        "\n",
        "# link for later reference: https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f\n",
        "# https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/\n",
        "# (after test, not before)\n",
        "\n",
        "\n",
        "# Label Encoding (MNIST -> labels were labelEncoded) and Label Binarizer\n",
        "# Dense multiple Sigmoids , each sigmoid is different for different objects\n",
        "# CONFIDENCE INTERVALS \n",
        "# result set = [p(Ball), p(bat), p(helmet), p(glove) ]\n",
        "# all above are independent probabilites and the sum of this set may not be equal to 1! \n",
        "\n",
        "# another primitive but popular way to do this is probability distribution (PandaVGG, ResNet)\n",
        "\n",
        "# image contains ball and bat ->    the last layer will be a SOFTMAX of results with distributions\n",
        "# nearly equal between ball and bat -> 4 classes-> bat, ball, gloves, helmet \n",
        "# image -> model -> [ 0.24, 0.64, 0.06, 0.06 ]\n",
        "\n",
        "\n",
        "\n",
        "# ACTIVATION of SIGMOID and SOFTMAX does EXACTLY the same thing, except SIGMOID has \n",
        "# a short-hard formula to get results faster- \n",
        "# SIGMOID is SOFTMAX-multiclass\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O0niUBJ3hXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Embedding(VOCAB_SIZE, 16, input_length=sentence_size)\n",
        "# the above line would create a matrix, with VOCAB_SIZE rows, and sentence_size columns\n",
        "\n",
        "# ranks: I:33, Going:13, am:5, home:24, today:44\n",
        "# step 1 of embedding layer \n",
        "#        <PAD>  <PAD> <PAD>  today  i   home \n",
        "# i          0   0     0      0     1    0\n",
        "#  going     0   0     0      0     0    0\n",
        "#  am        0   0     0      0     0    0\n",
        "#  home      0   0     0      0     0    1\n",
        "#  today     0   0     0      1     0    0\n",
        "# step 2: multiply this matrix with the weights or ranks of the words of sentence \n",
        "#        <PAD>  <PAD> <PAD>  today  i   home                \n",
        "# i          0   0     0      0     1    0                          33\n",
        "#  going     0   0     0      0     0    0          X Ranks =        0\n",
        "#  am        0   0     0      0     0    0                           0 \n",
        "#  home      0   0     0      0     0    1                          24\n",
        "#  today     0   0     0      1     0    0                          44\n",
        "\n",
        "# the sequence generate at the end, is broken into OUTPUT_DIMENSIONS and \n",
        "# passed as features to next layer\n",
        "# these broken OUTPUT_DIMENSIONS are now called VECTORS. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJm5lRA25NLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tensorflow\n",
        "\n",
        "# TENSORFLOW SESSION is just like a db session in networking ways\n",
        "# you need to open session, execute and close the session. \n",
        "# tf.Variable, tf.Placeholder help you create model without actual data till you\n",
        "# need to run it (that's when you need actual data)\n",
        "\n",
        "# tf.Placeholder -> remains constant across epochs\n",
        "# tf.Variable -> changes per epoch or per step\n",
        "\n",
        "# LAZY EVALUATION -> why? because tf is trying to draw a directed acyclic graph.. \n",
        "# if not done properly, it will defeat the purpose of using a graph data structure \n",
        "# in the first place\n",
        "# we depend on TF to draw this DAG for us\n",
        "# hence TF should be given ALL info before execution can begin. \n",
        "\n",
        "# what if we want immediate results? (like numpy or torch!)\n",
        "# EAGER EXECUTION or EVALUATION! DOn't create a DAG-> solve the matrices as if \n",
        "# they were numpy array \n",
        "\n",
        "# don't use deep learning, use shallow neurals to machine learn instead \n",
        "# tf.ESTIMATOR (machine learning pre-trained models are called estimators, like Linear Regression)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1alknH9O7Y6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Good variables and bad variables \n",
        "\n",
        "# REMEBER BCA problem! ID was an example! \n",
        "\n",
        "# 1) Correlation via heatmap -> told us strong and weak variables \n",
        "# 2) Scatter plot -> variables' plotted w.r.t. output distribution (tells if a variable was\n",
        "# distinct enough to be used- labels should have at least some unique area)\n",
        "# 3) DistPlot -> visualize the above distribution better to see what happened \n",
        "# 4) Autocorrelation -> variable v/s itself (but shifted on time-axis)\n",
        "# 5) multicollinearity -> multiple variables that were strongly correlated should be removed \n",
        "# or transformed. WHy? because it will lead to model heavily overfitted towards a REDUNDANT \n",
        "# variable\n",
        "# y = w1x1 + w2x2 + w3x3 + w4x4 + bias \n",
        "# now we say, that x2 and x3 and x4 were correlated as 1\n",
        "# p(y) = p( w1x1 + w2x2 + w3x2 + w4x2 ) + bias \n",
        "# all x2 correlations would have rendered x1 useless! \n",
        "\n",
        "# 6) outlier detection -> Z-Score, T-score \n",
        "# t-score is z-score calculation but strictly on a fixed scale \n",
        "# https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/hypothesis-testing/t-score-vs-z-score/\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arN5rdaoQynl",
        "colab_type": "text"
      },
      "source": [
        "#Markov Chains (Not in test)\n",
        "\n",
        "Whenever probabilistic problems arise, we need a model that gives us the probability which can be thought of a regression between 0 and 1. \n",
        "\n",
        "Till now, we had 1 set of algorithms called Naive Bayes' that were used along with bayesian principles to infer. \n",
        "\n",
        "\n",
        "NAIVE -> assumption all variables are independent \n",
        "\n",
        "Need of a mature model that could manage states\n",
        "\n",
        "State Diagram -> Markov Chain \n",
        "\n",
        "Markov Matrix\n",
        "\n",
        "           Eat     -   Sleep    - Play\n",
        "\n",
        "Eat        0.4         0.2        0.4 \n",
        "\n",
        "Sleep      0.2        0.2        0.6\n",
        "\n",
        "Play       0.4         0.6         0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Primary application: Currency exchange rates, genetics, seasons and temperatures, Google's Page Rank algorithm \n",
        "\n",
        "https://www.datacamp.com/community/tutorials/markov-chains-python-tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE8DUQQXUeOK",
        "colab_type": "text"
      },
      "source": [
        "Metrics:\n",
        "\n",
        "Mathematically, how do you decide the improvement? Is it a completion ratio? is it performance? is it time?\n",
        "\n",
        "It is all of them. \n",
        "\n",
        "TIME has been quantified with the Epochs. \n",
        "\n",
        "We can control execution time by batch_size and steps_per_epoch \n",
        "\n",
        "Examples: Accuracy, Precion, Recall, F1 Score, Mean Squared Error, Mean Absolute Error\n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75EUsW06VPt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CLEANING DATA (VERY HANDY GUIDES)\n",
        "# https://developers.google.com/machine-learning/crash-course/representation/feature-engineering\n",
        "# https://developers.google.com/machine-learning/crash-course/representation/qualities-of-good-features\n",
        "# https://developers.google.com/machine-learning/crash-course/representation/cleaning-data\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}